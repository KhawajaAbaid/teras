{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing missing data in teras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Using state of the art deep learning data imputation models for tabular data can be quite a challenge, not just because of how complex the model architecture might get, but also because of the data preprocessing and transformation steps involved. But teras makes it as easy as doing a classification or regression task.\n",
    "\n",
    "\n",
    "As of teras v0.3, it offers two GAN-based architectures for data imputation, namely ``GAIN`` and ``PCGAIN``.\n",
    "\n",
    "For the sake of this tutorial, we'll use the ``GAIN`` architecture.\n",
    "\n",
    "\n",
    "So without further ado, let's get to coding!\n",
    "\n",
    "As always, the first step is to configure your backend. I'll be using JAX because it's almost always is the fastest of the three.\n",
    "\n",
    "To configure your backend for teras, you need to set the ``KERAS_BACKEND`` environment variable.\n",
    "\n",
    "**NOTE:** You need to configure you backend before importing ``teras``/``keras``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we'll be using the Boston Housing dataset made available by keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import boston_housing\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine all the data since our task here is self-supervised so we don't need labels or test data to compute any metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 14)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dataset = np.concatenate([np.concatenate([X_train, y_train[:, np.newaxis]], axis=1),\n",
    "                          np.concatenate([X_test, y_test[:, np.newaxis]], axis=1)],\n",
    "                         axis=0)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always a good idea to normalize our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "normalizer = Normalizer()\n",
    "dataset = normalizer.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this dataset in itself doesn't contain any missing value, so we'll inject missing values ourselves to simulate a real world scenario.\n",
    "\n",
    "And for that, teras offers a handy utility that can be quite helpful for quickly simulating such situations. It conveniently named ``inject_missing_values``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of missing values: \n",
      "Before injecting:  0\n",
      "After injecting:  1426\n"
     ]
    }
   ],
   "source": [
    "from teras.utils import inject_missing_values\n",
    "\n",
    "print(\"# of missing values: \")\n",
    "print(\"Before injecting: \", np.isnan(dataset).sum())\n",
    "dataset = inject_missing_values(dataset, 0.2)\n",
    "print(\"After injecting: \", np.isnan(dataset).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``GAIN`` architecture that we'll be using requires dataset in the form ``(x_generator, x_discriminator)``. \n",
    "\n",
    "There's a handy data utility function in teras for this purpose named ``create_gain_dataset``.\n",
    "\n",
    "``NOTE:`` As of teras v0.3.0, you need to have TensorFlow installed to use this function since it makes use of ``tf.data`` to create a TensorFlow dataset that is then handled by Keras 3 to be used with any backend.\n",
    "It is also true for any data sampling classes available in teras. You may not like TensorFlow but you cannot not like ``tf.data``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 13:43:50.949841: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-10 13:43:50.949885: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-10 13:43:50.951338: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-10 13:43:52.014177: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from teras.data_utils import create_gain_dataset\n",
    "\n",
    "gain_dataset = create_gain_dataset(dataset)\n",
    "\n",
    "# Remember to batch your tensorflow dataset\n",
    "BATCH_SIZE = 64\n",
    "gain_dataset = gain_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import ``GAIN``\n",
    "\n",
    "Since ``GAIN`` is a generative adversarial network, so it requires a instaces of a generator and a discriminator, which we'll also import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from teras.models import GAIN\n",
    "from teras.models import GAINGenerator\n",
    "from teras.models import GAINDiscriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the documentation, to instantiate either the ``GAINGenerator`` or ``GAINDiscriminator`` you need a positional argument namely ``data_dim``.\n",
    "Now it's usually the same as the input dimensionality of the dataset, but is named so for cases when the input dataset has different dimensionality from the original dataset due to data transformations and such other preprocessing craft.\n",
    "\n",
    "Anyway, here ``data_dim`` refers to the dimensionality of the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = GAINGenerator(data_dim=dataset.shape[1])\n",
    "\n",
    "discriminator = GAINDiscriminator(data_dim=dataset.shape[1])\n",
    "\n",
    "gain = GAIN(generator,\n",
    "            discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** You can customize these models futher by specifying various keyword arguments. Look up docs! I'll just stick with default for the sake of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compile our model. Note that we're not passing any loss function to the compile method of ``GAIN`` instance, the reason being these specialized architectures contain loss computing methods within."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "gain.compile(generator_optimizer=keras.optimizers.Adam(),\n",
    "             discriminator_optimizer=keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rule of thumb for GAN-based models in teras is to ALWAYS build them yourself because the dataset that we pass to such architectures is usually deviates from normal (X, y) paired dataset, so Keras fails to build such models automatically due to failure to infer expected input shape.\n",
    "\n",
    "So let's build the model ourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain.build((BATCH_SIZE, dataset.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if and only if you're using the JAX backend, you'll have to call the `build_optimizers` method when using any GAN based model or any model that makes use of more than one optimizer. It is not needed for other backends like TensorFlow or PyTorch, neither it is needed for any architecture that only uses a single optimizer, which is usually how it is in 99.99% of the cases.\n",
    "\n",
    "Anyway, since we ARE using the JAX backend, so we'll call this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain.build_optimizers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING:** Calling ``build_optimizers`` method on a backend other than JAX will result in error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 384ms/step - discriminator_loss: 0.7368 - generator_loss: 48.5096\n",
      "Epoch 2/2\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - discriminator_loss: 0.7002 - generator_loss: 47.1353\n"
     ]
    }
   ],
   "source": [
    "history = gain.fit(gain_dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model is trained. Cool. But if we can't put it to use, it's useless. So let's put it into use.\n",
    "\n",
    "To impute data with missing values, you can either use the ``predict`` method of the trained ``GAIN`` instance or use a the ``Imputer`` class available in ``teras.tasks`` module. The ``Imputer`` class may not be that useful here, but it can be very useful in cases where you transform your data using a data transformer class.\n",
    "\n",
    "So, assuming you already know how to use ``predict``, we'll use the ``Imputer`` class here. It offers an ``impute`` method that takes in dataset with missing values and returns imputed data. If a data transformer instance is passed in during the instantiation, it will return the imputed data in its original format.\n",
    "\n",
    "Since we're not using any data transformer class so we'll set the ``reverse_transform`` parameter in ``impute`` method to ``False`` otherwise it'll result in error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n"
     ]
    }
   ],
   "source": [
    "from teras.tasks import Imputer\n",
    "\n",
    "gain_imputer = Imputer(gain)\n",
    "\n",
    "imputed_dataset = gain_imputer.impute(dataset, reverse_transform=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in the original dataset:  1426\n",
      "Missing values in the imputed dataset:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing values in the original dataset: \", np.isnan(dataset).sum())\n",
    "print(\"Missing values in the imputed dataset: \", np.isnan(imputed_dataset).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that wraps it up! As you saw, it's super easy and intuitive to use state of the art complex architectures for data imputation, thanks to teras!\n",
    "\n",
    "If you have any questions or run into an issue, reach us at twitter \n",
    "[@TerasML](https://twitter.com/TerasML) or file an issue at [teras github repository](https://github.com/KhawajaAbaid/teras)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
