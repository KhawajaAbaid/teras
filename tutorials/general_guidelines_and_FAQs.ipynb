{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Guidelines and FAQs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What's features metadata and why do we need it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some layers like `CategoricalFeatureEmbedding` that need to be able to differentiate between feature types and apply certain operations to relevant features, need a way for that differentiation.\n",
    "\n",
    "And for that very purpose, we've introduced this concept of **Features Metadata** which is a dictionary that contains two sub dictionaries, one for categorical features and one for numerical features. \n",
    "\n",
    "The categorical features dictionary maps the categorical feature names to a tuple of feature index in the dataset and a list of unique values in that features i.e. `{feature_name: (feature_index, list_of_unique_values)}`. The list of unique values within a categorical feature is also sometimes referred to as the \"*vocabulary*\".\n",
    "It can be accessed through `features_metadata[\"categorical\"]`\n",
    "\n",
    "The numerical features dictionary maps the numerical feature names to the feature index in the dataset i.e. `{feature_name: feature_index}`. \n",
    "It can be accessed through `features_metadata[\"numerical\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Why does the categorical features dictionary contains a list of unique values for each feature? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why categorical features dictionary contains a list of unqiue values for each feature is that, in case the user wants to encode the string/categorical values in the features, we want to have a lookup table to map these categorical values to their integer representations (here categorical values are mapped to their indices).\n",
    "\n",
    "And we want to construct that lookup table on all of the categorical values, not just on values that exist within a batch of data which is what we have access to during training — since as you may guess the batch of data is likely to miss many of the categorical values that exist within the dataset and creating lookup table over just those values will result in unexpected and errornous results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Using Teras utility to get features metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, you don't need to worry about constructing that features metadata dictionary yourself, `Teras` offers a handy utility function just for that purpose!\n",
    "\n",
    "Just import the `get_features_metadata_for_embedding` function from the `teras.utils` module and pass it the dataset in `pandas DataFrame` format along with a list of categorical features names — if they don't exist in your dataset just leave it as `None`, and a list of numerial features names — again if they don't exist then leave it as `None`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Converting your dataframe to tensorflow dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you're done with preprocessing, you must convert your pandas DataFrame to a tensorflow dataset.\n",
    "\n",
    "Here you should be mindful of two things:\n",
    "1. If your dataset contains heterogenuos data, i.e. it contains numerical features and categorical features that are in string format, you must create a dictionary format tensorflow dataset. (Skip ahead to see how to do that!)\n",
    "2. If you dataset contains homogenous data, i.e. it contains numerical features and categorical features that are already encoded to integers/floats, you can either create a dictionary format tensorflow dataset or the default array format tensorflow dataset. Though, in this case it is recommended to create an array format one.\n",
    "\n",
    "\n",
    "Don't worry too much on how to do that, `Teras` makes it easy for you do create a dataset of either format.\n",
    "\n",
    "Just import `dataframe_to_tf_dataset` function from `teras.utils` and pass it your dataframe, along with optional `target`, `batch_size` and `as_dict` parameters.\n",
    "\n",
    "Now, remember the two different types of tensorflow datasets, well `as_dict` determines the type of tensorflow dataset that is created. If it is set to True, a dictionary format tensorflow dataset is created, otherwise an array foramt tensorflow dataset is created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Teras's Parametric vs LayerFlow API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teras offers two different APIs for accesing and customizing the models to satiate different levels of accessibility and flexibility needs.\n",
    "\n",
    "1. **Parametric API**: It is the default API and something you're already familiar with — you import the model class and specify the values for parameters that determine how the sub layers, models or blocks within the given model are constructed. \n",
    "\n",
    "    For instance, specify the `embedding_dim` parameter during instantiation of `TabTransformerClassifier` and that will be the dimensionality value used to construct/instatiate the `CategoricalFeatureEmbedding` layer. Simple enough, right?\n",
    "\n",
    "\n",
    "2. **LayerFlow API**: It maximizes the flexbility while minimizing the interface. That may sound a bit contradictory at first, but let me explain. Here, instead of specifiy individual parameters specific to sub layers/models, the user instead specifies instances of those sub layers, models or blocks that are used in the given model architecture.\n",
    "\n",
    "    For instance, instead of specifying the `embedding_dim` parameter value, the user specifies an instance of `CategoricalFeatureEmbedding` layer. \n",
    "\n",
    "\n",
    "    Now in this instance, we're just passing one parameter instead of another so it may not seem like much beneficial at first glance but let me highlight how it can immensely help depending on your use case:\n",
    "\n",
    "    1. Since all you need to pass is an instance of layer, it can be any layer, there's no resitriction that it must be instance of `CategoricalFeatureEmbedding` layer — which means that you get complete control over not just customizing the existing layers offered by Teras but also you can design/create an Embedding layer of your own that can work in the place of the original  `CategoricalFeatureEmbedding` layer or any other layer for that matter. This is especially useful, if you're a desinging a new architecture and want to rapidly test out new modifications of the existing architectures by just plugging certain custom layers of your own in place of the default ones. Pretty cool, right?\n",
    "\n",
    "    2. In many cases, to reduce the plethora of parameters and keep the most important ones, some parameters specific to sub-layers, models are not offered at the top level of the given architecture by the Parametric API, so if you need to tweak those parameters missing from the main model, you can use LayerFlow API and create an instance of that layer/model with desired parameters and pass it to the model.\n",
    "\n",
    "    Now, some of you mega smart minded ones, might be thinking, what if there was a way to pass instances of certain layers but also just specify some parameter value for other layers and leave it to `Teras` to instantiate those layers, well I'm just like you fr and you can do just that with the LayerFlow API. Even though, the parameters specific to layers aren't inlcuded in the documentation of the LayerFlow version of those layers/models, but you can specify any parameter that is offered by the **Parametric API** in the **LayerFlow API** version of the model.\n",
    "    \n",
    "    For instance, say we just want to pass an instance of our custom Categorical Embedding layer but instead of specify an instance of the `Encoder` layer, we just want to modify the `dropout rate` for the `FeedForward` layer within it. Well since we know that the `TabTransformerClassifier`'s `Parametric` version exposes a `feed_forward_dropout` parameter, we can pass that keyword argument in the `LayerFlow` verson of the `TabTransformerClassifier`.\n",
    "\n",
    "    Here's how you'd do it in the code:\n",
    "    ```\n",
    "    from teras.layerflow.models import TabTransformerClassifier\n",
    "\n",
    "    custom_categorical_embedding = CustomCategoricalFeatureEmbedding()\n",
    "\n",
    "    model = TabTransformerClassifier(categorical_feature_emebdding=custom_categorical_embedding,\n",
    "                                    feed_forward_dropout=0.5)\n",
    "    ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compiling Pretraining and Data Generation models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For highly customized architectures like pretrainer models like `TabNetPretrainer` or data generation arhitectures like `CTGAN`, which employ custom loss functions, `Teras` has default values for loss functions and optimizers in place.\n",
    "\n",
    "Though, you can specify any optimizer, **unless** you understand the underlying structure of the pretrainer arhcitecture, i*t's better to just use the default loss function*.\n",
    "\n",
    "As for metrics, you'd need to understand what that model returns at each batch to specify a custom metric designed just for that achitecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data Transformer and Data Sampler classes for Generative models\n",
    "The generative architectures like `GAIN`, `PCGAIN`, `CTGAN` and `TVAE` require sophisticated data preprocessing and transformation as well as how the batches of data are generated, hence to make it easier for the users, Teras implements `DataTransformer` and `DataSampler` classes for each of these models.\n",
    "\n",
    "These can be imported from the `teras.preprocessing.<architecture_name>` module.\n",
    "For instance, for GAIN, we'll import its `DataSampler` and `DataTransformer` classes as follows,\n",
    "\n",
    "`from teras.preprocessing.gain import DataSampler, DataTransformer`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
