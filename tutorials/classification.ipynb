{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teras Classificaiton Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "In this tutorial, we'll use Teras and its both API, default and LayerFlow for classification task.\n",
    "\n",
    "**Model**: `TabTransformerClassifier`\n",
    "\n",
    "**Dataset**: Adult Income dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt   education  educational-num      marital-status  \\\n",
       "0   25    Private  226802        11th                7       Never-married   \n",
       "1   38    Private   89814     HS-grad                9  Married-civ-spouse   \n",
       "2   28  Local-gov  336951  Assoc-acdm               12  Married-civ-spouse   \n",
       "\n",
       "          occupation relationship   race gender  capital-gain  capital-loss  \\\n",
       "0  Machine-op-inspct    Own-child  Black   Male             0             0   \n",
       "1    Farming-fishing      Husband  White   Male             0             0   \n",
       "2    Protective-serv      Husband  White   Male             0             0   \n",
       "\n",
       "   hours-per-week native-country income  \n",
       "0              40  United-States  <=50K  \n",
       "1              50  United-States  <=50K  \n",
       "2              40  United-States   >50K  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "adult_df = pd.read_csv(\"./datasets/adult_income_dataset.csv\")\n",
    "adult_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_feats = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country']\n",
    "numerical_feats = ['age', 'hours-per-week']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_df.replace(\"?\", np.nan, inplace=True)\n",
    "adult_df.loc[:, categorical_feats] = adult_df.loc[:, categorical_feats].fillna(\"missing\")\n",
    "adult_df.loc[:, numerical_feats] = adult_df.loc[:, numerical_feats].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult_df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encode the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "adult_df[\"income\"] = le.fit_transform(adult_df[\"income\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No need to encode the Categorical Features in the dataset!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you might wonder if we need to encode all the other categorical features in the dataset as well, since many of them contain string values, and we all know deep learning models require data to be in int/float format, right? Well, **No**, here you don't need to encode them.\n",
    "\n",
    "**But Why?**\n",
    "\n",
    "Because  here we're using `TabTransformer` model which is a **Transformer** based architecture for Classificaiton and Regression, it contains a `CategoricalFeatureEmbedding` layer which handles the categorical features, even if they contain string values. Just set the `encode` parameter to `True` and that layer will take care of encoding the string values.\n",
    "\n",
    "Just to make it clear, here **encoding** means converting *string values to integers/floats* and **embedding** means converting those *encoded values to dense representations* of `embeddeing_dim` dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's features metadata and why do we need it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some layers like `CategoricalFeatureEmbedding` that need to be able to differentiate between feature types and apply certain operations to relevant features, need a way for that differentiation.\n",
    "\n",
    "And for that very purpose, we've introduced this concept of **Features Metadata** which is a dictionary that contains two sub dictionaries, one for categorical features and one for numerical features. \n",
    "\n",
    "The categorical features dictionary maps the categorical feature names to a tuple of feature index in the dataset and a list of unique values in that features i.e. `{feature_name: (feature_index, list_of_unique_values)}`. The list of unique values within a categorical feature is also sometimes referred to as the \"*vocabulary*\".\n",
    "It can be accessed through `features_metadata[\"categorical\"]`\n",
    "\n",
    "The numerical features dictionary maps the numerical feature names to the feature index in the dataset i.e. `{feature_name: feature_index}`. \n",
    "It can be accessed through `features_metadata[\"numerical\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why does the categorical features dictionary contains a list of unique values for each feature? \n",
    "The reason why categorical features dictionary contains a list of unqiue values for each feature is that, in case the user wants to encode the string/categorical values in the features, we want to have a lookup table to map these categorical values to their integer representations (here categorical values are mapped to their indices).\n",
    "\n",
    "And we want to construct that lookup table on all of the categorical values, not just on values that exist within a batch of data which is what we have access to during training — since as you may guess the batch of data is likely to miss many of the categorical values that exist within the dataset and creating lookup table over just those values will result in unexpected and errornous results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We've got you covered!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, you don't need to worry about constructing that features metadata dictionary yourself, `Teras` offers a handy utility function just for that purpose!\n",
    "\n",
    "Just import the `get_features_metadata_for_embedding` function from the `teras.utils` module and pass it the dataset in `pandas DataFrame` format along with a list of categorical features names — if they don't exist in your dataset just leave it as `None`, and a list of numerial features names — again if they don't exist then leave it as `None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from teras.utils import get_features_metadata_for_embedding\n",
    "\n",
    "metadata = get_features_metadata_for_embedding(adult_df,\n",
    "                                               categorical_features=categorical_feats,\n",
    "                                               numerical_features=numerical_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert dataframe to tensorflow dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after you're done with preprocessing, you must convert your pandas DataFrame to a tensorflow dataset.\n",
    "\n",
    "Here you should be mindful of two things:\n",
    "1. If your dataset contains heterogenuos data, i.e. it contains numerical features and categorical features that are in string format, you must create a dictionary format tensorflow dataset. (Skip ahead to see how to do that!)\n",
    "2. If you dataset contains homogenous data, i.e. it contains numerical features and categorical features that are already encoded to integers/floats, you can either create a dictionary format tensorflow dataset or the default array format tensorflow dataset. Though, in this case it is recommended to create an array format one.\n",
    "\n",
    "\n",
    "Don't worry too much on how to do that, `Teras` makes it easy for you do create a dataset of either format.\n",
    "\n",
    "Just import `dataframe_to_tf_dataset` function from `teras.utils` and pass it your dataframe, along with optional `target`, `batch_size` and `as_dict` parameters.\n",
    "\n",
    "Now, remember the two different types of tensorflow datasets, well `as_dict` determines the type of tensorflow dataset that is created. If it is set to True, a dictionary format tensorflow dataset is created, otherwise an array foramt tensorflow dataset is created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our dataset contains categroical features with string values, so we'll create a dictionary format tensorflow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from teras.utils import dataframe_to_tf_dataset\n",
    "\n",
    "adult_ds = dataframe_to_tf_dataset(dataframe=adult_df,\n",
    "                                   target=\"income\",\n",
    "                                   as_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's pretty much it for the preprocessing!\n",
    "\n",
    "Notice, how much preprocessing code is cut short and is not needed for a Teras model, as compared to a typical machine leanring model such as SVM or tree based models.\n",
    "\n",
    "Though it does need to be mentioned that, although the statement above holds true for most advanced architectures like transformer based models, some other architectures offered by `Teras` may require the categorical features to be encoded by user during preprocessing step, such as `DNFNetClassifier`, `NODEClassifier` etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and instantiating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teras offers two different APIs for accesing and customizing the models to satiate different levels of accessibility and flexibility needs.\n",
    "\n",
    "1. **Parametric API**: It is the default API and something you're already familiar with — you import the model class and specify the values for parameters that determine how the sub layers, models or blocks within the given model are constructed. \n",
    "\n",
    "    For instance, specify the `embedding_dim` parameter during instantiation of `TabTransformerClassifier` and that will be the dimensionality value used to construct/instatiate the `CategoricalFeatureEmbedding` layer. Simple enough, right?\n",
    "\n",
    "\n",
    "2. **LayerFlow API**: It maximizes the flexbility while minimizing the interface. That may sound a bit contradictory at first, but let me explain. Here, instead of specifiy individual parameters specific to sub layers/models, the user instead specifies instances of those sub layers, models or blocks that are used in the given model architecture.\n",
    "\n",
    "    For instance, instead of specifying the `embedding_dim` parameter value, the user specifies an instance of `CategoricalFeatureEmbedding` layer. \n",
    "\n",
    "\n",
    "    Now in this instance, we're just passing one parameter instead of another so it may not seem like much beneficial at first glance but let me highlight how it can immensely help depending on your use case:\n",
    "\n",
    "    1. Since all you need to pass is an instance of layer, it can be any layer, there's no resitriction that it must be instance of `CategoricalFeatureEmbedding` layer — which means that you get complete control over not just customizing the existing layers offered by Teras but also you can design/create an Embedding layer of your own that can work in the place of the original  `CategoricalFeatureEmbedding` layer or any other layer for that matter. This is especially useful, if you're a desinging a new architecture and want to rapidly test out new modifications of the existing architectures by just plugging certain custom layers of your own in place of the default ones. Pretty cool, right?\n",
    "\n",
    "    2. In many cases, to reduce the plethora of parameters and keep the most important ones, some parameters specific to sub-layers, models are not offered at the top level of the given architecture by the Parametric API, so if you need to tweak those parameters missing from the main model, you can use LayerFlow API and create an instance of that layer/model with desired parameters and pass it to the model.\n",
    "\n",
    "    Now, some of you mega smart minded ones, might be thinking, what if there was a way to pass instances of certain layers but also just specify some parameter value for other layers and leave it to `Teras` to instantiate those layers, well I'm just like you fr and you can do just that with the LayerFlow API. Even though, the parameters specific to layers aren't inlcuded in the documentation of the LayerFlow version of those layers/models, but you can specify any parameter that is offered by the **Parametric API** in the **LayerFlow API** version of the model.\n",
    "    \n",
    "    For instance, say we just want to pass an instance of our custom Categorical Embedding layer but instead of specify an instance of the `Encoder` layer, we just want to modify the `dropout rate` for the `FeedForward` layer within it. Well since we know that the `TabTransformerClassifier`'s `Parametric` version exposes a `feed_forward_dropout` parameter, we can pass that keyword argument in the `LayerFlow` verson of the `TabTransformerClassifier`.\n",
    "\n",
    "    Here's how you'd do it in the code:\n",
    "    ```\n",
    "    from teras.layerflow.models import TabTransformerClassifier\n",
    "\n",
    "    custom_categorical_embedding = CustomCategoricalFeatureEmbedding()\n",
    "\n",
    "    model = TabTransformerClassifier(categorical_feature_emebdding=custom_categorical_embedding,\n",
    "                                    feed_forward_dropout=0.5)\n",
    "    ```\n",
    "\n",
    "    Now enough with theory, let's put things into practice!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using default API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 44s 559ms/step - loss: 0.4013 - accuracy: 0.8061\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from teras.models import TabTransformerClassifier\n",
    "\n",
    "model = TabTransformerClassifier(num_classes=2,\n",
    "                                features_metadata=metadata)\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "              loss=keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(adult_ds, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LayerFlow API\n",
    "Let's now use LayerFlow API and customize the `head` layer a bit before plugging it with the rest of the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abaid/miniconda3/envs/ML/lib/python3.10/site-packages/numpy/core/numeric.py:2463: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 42s 541ms/step - loss: 0.6360 - accuracy: 0.7256\n"
     ]
    }
   ],
   "source": [
    "from teras.layerflow.models import TabTransformerClassifier\n",
    "# We've shortend the TabTransformer<LayerName> to TabT<LayerName> \n",
    "# to make it easier for imports and use.\n",
    "# Let us know if that sounds confusing or if you've got other ideas for feedback!\n",
    "from teras.layerflow.layers import TabTClassificationHead\n",
    "\n",
    "# Say we want to use a custom hidden block for the TabTClassificationHead\n",
    "# but want to use the default output layer\n",
    "# Here, we'll just use a simple dense layer with relu activation\n",
    "# as the hidden block\n",
    "head_hidden_block = keras.layers.Dense(32, activation=\"relu\")\n",
    "\n",
    "# Note that, since TabTClassificaitonHead is from LayerFlow API,\n",
    "# it doesn't direclty expose the num_classes parameter,\n",
    "# but we know that the Parametric version of it does expose\n",
    "# this parameter, hence we specify it's value so `Teras`\n",
    "# takes care of creating an default output layer but with\n",
    "# appropriate number of classes.\n",
    "head = TabTClassificationHead(hidden_block=head_hidden_block,\n",
    "                              num_classes=2)\n",
    "\n",
    "model_lf = TabTransformerClassifier(features_metadata=metadata,\n",
    "                                    head=head)\n",
    "model_lf.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "                 loss=keras.losses.BinaryCrossentropy(),\n",
    "                 metrics=\"accuracy\")\n",
    "history = model_lf.fit(adult_ds, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrappin it up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that wraps up our classification tutorial using Teras.\n",
    "If you need more help, consult documentation, and other available resources and if that still leaves you with questions, feel free to raise an issue or email me khawaja.abaid@gmail.com\n",
    "\n",
    "If you find `Teras` useful, please consider giving it a star on GitHub and sharing it with others! Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
